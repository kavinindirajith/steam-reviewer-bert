{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.read_csv(\"../data/reviews.csv\")\n",
    "\n",
    "reviews = df['review'].tolist()\n",
    "labels = df['voted_up'].tolist()\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure all elements in train_texts and test_texts are strings\n",
    "train_texts = [str(text) for text in train_texts]\n",
    "test_texts = [str(text) for text in test_texts]\n",
    "\n",
    "# Clean reviews to remove unusual characters\n",
    "train_texts = [re.sub(r'[^\\x00-\\x7F]+', ' ', text) for text in train_texts]\n",
    "test_texts = [re.sub(r'[^\\x00-\\x7F]+', ' ', text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Check if the device supports MPS (Metal Performance Shaders) for Apple Silicon or fallback to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load BERT tokenizer and pre-trained model for sequence classification (binary classification)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the training and test texts\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "# Convert encodings and labels to TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Set up optimizer with weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.2030\n",
      "Epoch 2/3, Loss: 0.1156\n",
      "Epoch 3/3, Loss: 0.0591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../model/fine_tuned_bert_reviews/tokenizer_config.json',\n",
       " '../model/fine_tuned_bert_reviews/special_tokens_map.json',\n",
       " '../model/fine_tuned_bert_reviews/vocab.txt',\n",
       " '../model/fine_tuned_bert_reviews/added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Training the BERT model for 3 epochs\n",
    "# epochs = 3\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in train_dataloader:\n",
    "#         # Unpack the batch and move tensors to device\n",
    "#         batch_input_ids, batch_attention_mask, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "\n",
    "#         # Loss is automatically computed by the model when `labels` are provided\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # Backward pass and optimization step\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "# # Save the fine-tuned model and tokenizer\n",
    "# model.save_pretrained('../model/fine_tuned_bert_reviews')\n",
    "# tokenizer.save_pretrained('../model/fine_tuned_bert_reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.95\n",
      "Recall: 0.97\n",
      "F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# No need to compute gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Unpack the batch and move tensors to device\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get predictions and store them\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        label_ids = batch_labels.cpu().numpy()\n",
    "\n",
    "        # Append results for comparison\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.95\n",
      "Recall: 0.97\n",
      "F1-Score: 0.96\n",
      "Confusion Matrix:\n",
      "[[ 836  288]\n",
      " [ 161 5067]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate various metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new, unseen reviews\n",
    "df2 = pd.read_csv(\"../data/new_reviews.csv\")\n",
    "\n",
    "reviews = df2['review'].tolist()\n",
    "true_labels = df2['voted_up'].tolist() \n",
    "\n",
    "reviews = [str(text) for text in reviews]\n",
    "reviews = [re.sub(r'[^\\x00-\\x7F]+', ' ', text) for text in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "batch_size = 2  # Reduced batch size\n",
    "max_length = 64  # Reduced sequence length\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Process in smaller batches\n",
    "for i in range(0, len(reviews), batch_size):\n",
    "    # Tokenize the current batch of reviews\n",
    "    encodings_chunk = tokenizer(reviews[i:i + batch_size], truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    # Move inputs to the correct device\n",
    "    input_ids_chunk = encodings_chunk['input_ids'].to(device)\n",
    "    attention_mask_chunk = encodings_chunk['attention_mask'].to(device)\n",
    "\n",
    "    # Perform inference with no gradient calculation to save memory\n",
    "    with torch.no_grad():\n",
    "        outputs_chunk = model(input_ids=input_ids_chunk, attention_mask=attention_mask_chunk)\n",
    "        logits_chunk = outputs_chunk.logits\n",
    "\n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predictions_chunk = torch.argmax(logits_chunk, dim=-1).cpu().numpy()\n",
    "    predictions.extend(predictions_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.97\n",
      "Recall: 0.99\n",
      "F1-Score: 0.98\n",
      "Confusion Matrix:\n",
      "[[1031  171]\n",
      " [  67 5491]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, F1-score\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: If you like long drives on straight roads going past farms, this is the DLC for you. Besides all the farms and small towns, there are some deliveries to be made to gas stations.\n",
      "\n",
      "Nebraska just doesn't have any real landmarks, but if you go in expecting a farming state, you get that. What there is to map has been mapped and it looks really good.\n",
      "\n",
      "After playing some more and doing the entire World of Trucks event to explore the state I have some issues though. None of the scenic towns have town markers! I passed through places like Red Cloud and Thedford but they don't have a town sign anywhere, and aren't mentioned on the road shields.\n",
      "\n",
      "The alignment of some roads is also off. The 6 in Lincoln for example manages to run north-south for a part, whereas IRL it is almost entirely a south-west to north-east road.\n",
      "\n",
      "I guess some of this may be corrected later but it seems this DLC was not completely finished before release.\n",
      "\n",
      "Still, if you like ATS, you will probably get this now if not soon simply for the fact it gives you more to drive on, and allows for more connections between Wyoming, Colorado, and Kansas.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: Good DLC, but will be better when the east side of the Missouri river is complete.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: I've seen an entire cluster of ants move a rock before, not a big rock... but like, a decent sized pebble for-\n",
      "*cough* \"I Saw Them Pick Up  \"\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: After driving through the whole state. I feel this DLC could do with a few more misc highways around the map. You missed Lincoln Highway (US 30) from Grand Island to North Platte which is a scenic alternative to the boringness of I-80. The tri-cities are a major area of Nebraska that weren't done justice here. Sure, Grand Island looks great but Kearney isn't represented outside of the Archway and Hastings doesn't even exist, which is crazy to me since Hastings is where kool-aid was invented.\n",
      "\n",
      "What little is here is good though. I just hope SCS could update the DLC with some more roads.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: Game crashes. cant use dealers, Cant use menu while in delivery without crashing, horrible update. With new current update, Especially With the Nebraska DLCi have every mod which is only all updated from steam.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Review: The tracks are foreboding, menacing....extremely atmospheric and very dangerous. Dangerous in the sense that this score sets the standard so high I think it will leave other scores in ruines. Just listen to it!\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: nice\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: A fun rhythm game played as crazy pigeons. I am having fun with it. The only problem if its really much of a problem. People need to learn to have some patience! Their timer at the start is 2 minutes or 30 people/pigeons. It will get down to 10 seconds and people still leave. If you don't mind playing against bots most rounds, then it's not really an issue. The bots are not over powered at all I would say. I just thought I would mention this because this is a bit annoying right now. And the lobby doesn't seem to fill back up as people leave. I hope they fix that. All of their clothing items/voices/ect. are in game currency (crumbs), no money shop in game. They have a DLC for some outfits, and it's a decent amount of outfits. I like their way they are doing this game so far. If you have decent rhythm. Give it a go.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: White powered wig and funky rock clothes?! YES PLEASE\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Review: Now you can fly, fly!\n",
      "Predicted Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 results to verify\n",
    "label_map = {0: 'Negative', 1: 'Positive'}\n",
    "predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "for review, sentiment in zip(reviews[:10], predicted_labels[:10]):\n",
    "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
